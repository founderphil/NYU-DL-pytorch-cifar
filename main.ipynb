{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711ec086-3d32-4ecc-9d4f-567524c7b1e3",
      "metadata": {
        "id": "711ec086-3d32-4ecc-9d4f-567524c7b1e3"
      },
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/founderphil/NYU-DL-pytorch-cifar'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse"
      ],
      "metadata": {
        "id": "R2OOtXhbCBKG"
      },
      "id": "R2OOtXhbCBKG",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrrWXjYcLU3z",
        "outputId": "b4b87a4e-1f4b-47ef-c435-9004688f5b1f"
      },
      "id": "YrrWXjYcLU3z",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr  2 00:59:43 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0              48W / 400W |   1723MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBECxno7LZPu",
        "outputId": "a4cc0948-36ca-47eb-82b1-25b21cfdf903"
      },
      "id": "OBECxno7LZPu",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('./NYU-DL-pytorch-cifar/models/')\n",
        "from models import *"
      ],
      "metadata": {
        "id": "L88OrGr5CGCn"
      },
      "id": "L88OrGr5CGCn",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "9140727a-8ea8-42cd-bbf5-85be662fb226",
      "metadata": {
        "id": "9140727a-8ea8-42cd-bbf5-85be662fb226"
      },
      "outputs": [],
      "source": [
        "sys.path.append('./NYU-DL-pytorch-cifar/utils.py')\n",
        "from utils import progress_bar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wVqPcHeGCFHA",
        "outputId": "24125183-85fb-4679-d116-34af73ad29f4"
      },
      "id": "wVqPcHeGCFHA",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "1eae4b63-599d-41a7-95e5-ac066f0f5bb6",
      "metadata": {
        "id": "1eae4b63-599d-41a7-95e5-ac066f0f5bb6"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
        "\n",
        "args, _ = parser.parse_known_args()  # This will ignore unrecognized arguments\n",
        "\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5077a13d-c1c3-4174-bb35-92dca22210f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5077a13d-c1c3-4174-bb35-92dca22210f5",
        "outputId": "ce18b5ab-49b9-42c5-a641-26a734f18a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 13104025.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 32  # Reduced initial channels\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 32, layers[0])  # Adjusted channels\n",
        "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2)  # Adjusted channels\n",
        "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2)  # Adjusted channels\n",
        "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2)  # Adjusted channels\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)  # Adjusted to match reduced channels\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet18_modified(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create the model and move it to the appropriate device\n",
        "model_modified = resnet18_modified().to(device)\n",
        "\n",
        "# Now the summary should work without errors\n",
        "summary(model_modified, (3, 32, 32))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-V9ikhBUP-e",
        "outputId": "a0b14527-122c-4e5d-bc62-a0718de34521"
      },
      "id": "k-V9ikhBUP-e",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             864\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-5           [-1, 32, 32, 32]              64\n",
            "              ReLU-6           [-1, 32, 32, 32]               0\n",
            "            Conv2d-7           [-1, 32, 32, 32]           9,216\n",
            "       BatchNorm2d-8           [-1, 32, 32, 32]              64\n",
            "              ReLU-9           [-1, 32, 32, 32]               0\n",
            "       BasicBlock-10           [-1, 32, 32, 32]               0\n",
            "           Conv2d-11           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-12           [-1, 32, 32, 32]              64\n",
            "             ReLU-13           [-1, 32, 32, 32]               0\n",
            "           Conv2d-14           [-1, 32, 32, 32]           9,216\n",
            "      BatchNorm2d-15           [-1, 32, 32, 32]              64\n",
            "             ReLU-16           [-1, 32, 32, 32]               0\n",
            "       BasicBlock-17           [-1, 32, 32, 32]               0\n",
            "           Conv2d-18           [-1, 64, 16, 16]          18,432\n",
            "      BatchNorm2d-19           [-1, 64, 16, 16]             128\n",
            "             ReLU-20           [-1, 64, 16, 16]               0\n",
            "           Conv2d-21           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-22           [-1, 64, 16, 16]             128\n",
            "           Conv2d-23           [-1, 64, 16, 16]           2,048\n",
            "      BatchNorm2d-24           [-1, 64, 16, 16]             128\n",
            "             ReLU-25           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-26           [-1, 64, 16, 16]               0\n",
            "           Conv2d-27           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-28           [-1, 64, 16, 16]             128\n",
            "             ReLU-29           [-1, 64, 16, 16]               0\n",
            "           Conv2d-30           [-1, 64, 16, 16]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 16, 16]             128\n",
            "             ReLU-32           [-1, 64, 16, 16]               0\n",
            "       BasicBlock-33           [-1, 64, 16, 16]               0\n",
            "           Conv2d-34            [-1, 128, 8, 8]          73,728\n",
            "      BatchNorm2d-35            [-1, 128, 8, 8]             256\n",
            "             ReLU-36            [-1, 128, 8, 8]               0\n",
            "           Conv2d-37            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
            "           Conv2d-39            [-1, 128, 8, 8]           8,192\n",
            "      BatchNorm2d-40            [-1, 128, 8, 8]             256\n",
            "             ReLU-41            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-42            [-1, 128, 8, 8]               0\n",
            "           Conv2d-43            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-44            [-1, 128, 8, 8]             256\n",
            "             ReLU-45            [-1, 128, 8, 8]               0\n",
            "           Conv2d-46            [-1, 128, 8, 8]         147,456\n",
            "      BatchNorm2d-47            [-1, 128, 8, 8]             256\n",
            "             ReLU-48            [-1, 128, 8, 8]               0\n",
            "       BasicBlock-49            [-1, 128, 8, 8]               0\n",
            "           Conv2d-50            [-1, 256, 4, 4]         294,912\n",
            "      BatchNorm2d-51            [-1, 256, 4, 4]             512\n",
            "             ReLU-52            [-1, 256, 4, 4]               0\n",
            "           Conv2d-53            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-54            [-1, 256, 4, 4]             512\n",
            "           Conv2d-55            [-1, 256, 4, 4]          32,768\n",
            "      BatchNorm2d-56            [-1, 256, 4, 4]             512\n",
            "             ReLU-57            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-58            [-1, 256, 4, 4]               0\n",
            "           Conv2d-59            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-60            [-1, 256, 4, 4]             512\n",
            "             ReLU-61            [-1, 256, 4, 4]               0\n",
            "           Conv2d-62            [-1, 256, 4, 4]         589,824\n",
            "      BatchNorm2d-63            [-1, 256, 4, 4]             512\n",
            "             ReLU-64            [-1, 256, 4, 4]               0\n",
            "       BasicBlock-65            [-1, 256, 4, 4]               0\n",
            "AdaptiveAvgPool2d-66            [-1, 256, 1, 1]               0\n",
            "           Linear-67                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 2,797,610\n",
            "Trainable params: 2,797,610\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 7.75\n",
            "Params size (MB): 10.67\n",
            "Estimated Total Size (MB): 18.44\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "900e7db9-017e-4027-ab24-fcd9fb8df1b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "900e7db9-017e-4027-ab24-fcd9fb8df1b8",
        "outputId": "8a0a747d-ab00-4fd7-f9cb-6c27439172fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "# net = VGG('VGG19')\n",
        "net = ResNet18()\n",
        "# net = PreActResNet18()\n",
        "# net = GoogLeNet()\n",
        "# net = DenseNet121()\n",
        "# net = ResNeXt29_2x64d()\n",
        "# net = MobileNet()\n",
        "# net = MobileNetV2()\n",
        "# net = DPN92()\n",
        "# net = ShuffleNetG2()\n",
        "# net = SENet18()\n",
        "# net = ShuffleNetV2(1)\n",
        "# net = EfficientNetB0()\n",
        "# net = RegNetX_200MF()\n",
        "# net = SimpleDLA()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "if args.resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "# Assuming your model is wrapped in nn.DataParallel and named 'net'\n",
        "# and you are using a device like 'cuda' or 'cpu'\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = net.module.to(device)\n",
        "\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "yvZN5m_GDbjk",
        "outputId": "e9d3c442-5084-44f5-dce7-db9ca6a176dc"
      },
      "id": "yvZN5m_GDbjk",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1024 and 256x10)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-562bac2135b5>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-993b81350551>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m                 for hook_id, hook in (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x1024 and 256x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "98e70dbf-aa1d-49fe-8b1d-936e044d16ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98e70dbf-aa1d-49fe-8b1d-936e044d16ea",
        "outputId": "83ab618d-2caf-4b2e-80d0-1e09f94b4efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            " [================================================================>]  Step: 32ms | Tot: 12s181ms | Loss: 2.359 | Acc: 9.556% (4778/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 1\n",
            " [================================================================>]  Step: 37ms | Tot: 12s541ms | Loss: 2.359 | Acc: 9.568% (4784/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 2\n",
            " [================================================================>]  Step: 26ms | Tot: 12s610ms | Loss: 2.358 | Acc: 9.444% (4722/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 3\n",
            " [================================================================>]  Step: 22ms | Tot: 12s392ms | Loss: 2.359 | Acc: 9.638% (4819/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 4\n",
            " [================================================================>]  Step: 38ms | Tot: 12s663ms | Loss: 2.357 | Acc: 9.336% (4668/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 5\n",
            " [================================================================>]  Step: 28ms | Tot: 12s447ms | Loss: 2.358 | Acc: 9.436% (4718/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 6\n",
            " [================================================================>]  Step: 24ms | Tot: 12s457ms | Loss: 2.359 | Acc: 9.380% (4690/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 7\n",
            " [================================================================>]  Step: 25ms | Tot: 12s191ms | Loss: 2.359 | Acc: 9.516% (4758/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 8\n",
            " [================================================================>]  Step: 30ms | Tot: 12s195ms | Loss: 2.359 | Acc: 9.510% (4755/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n",
            "\n",
            "Epoch: 9\n",
            " [================================================================>]  Step: 29ms | Tot: 11s971ms | Loss: 2.359 | Acc: 9.394% (4697/50000)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 391/391 \n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    model_modified.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_modified(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    model_modified.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model_modified(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        " #           progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        " #                        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'model_modified': model_modified.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+10):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlQWQl4julXU",
        "outputId": "1db195ae-f4bf-48d5-b573-22e251b8870e"
      },
      "id": "UlQWQl4julXU",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mNYU-DL-pytorch-cifar\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "fb4b696b-7b5c-4019-b62a-bbf42dbcca5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "fb4b696b-7b5c-4019-b62a-bbf42dbcca5a",
        "outputId": "9b6dc029-78f7-4981-fd5e-5002199d7f3c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "pickle data was truncated",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-579f54ba576b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/cifar_test_nolabels.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load the dataset\n",
        "with open('./data/cifar_test_nolabels.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "    images = data['data'].reshape((-1, 3, 32, 32)).astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "unpickle('./cifar_test_nolabels.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ITO6hGr2Iu4",
        "outputId": "c5225c46-6c3a-406b-df11-e1f11845424f"
      },
      "id": "0ITO6hGr2Iu4",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{b'data': array([[133, 136, 136, ..., 226, 225, 224],\n",
              "        [160, 177, 176, ...,  89,  89,  88],\n",
              "        [255, 255, 255, ..., 211, 213, 215],\n",
              "        ...,\n",
              "        [ 29,  29,  45, ..., 156, 155, 154],\n",
              "        [124, 123, 126, ...,  49,  49,  51],\n",
              "        [255, 255, 255, ..., 250, 251, 255]], dtype=uint8),\n",
              " b'ids': array([   0,    1,    2, ..., 9997, 9998, 9999])}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = unpickle('./cifar_test_nolabels.pkl')\n",
        "image_id = 1111 #change this to look at the image by id (goes 0 thru 9999)\n",
        "\n",
        "an_image = data[b'data'][image_id]\n",
        "an_image = an_image.reshape(3, 32, 32)\n",
        "an_image = an_image.transpose(1, 2, 0)\n",
        "\n",
        "\n",
        "plt.imshow(an_image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "WXmGFXAE5Iov",
        "outputId": "94e69d0b-d102-4783-ba62-33a827cb96dc"
      },
      "id": "WXmGFXAE5Iov",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(CustomResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def custom_resnet(num_classes=10):\n",
        "    return CustomResNet(BasicBlock, [3, 3, 3], num_classes=num_classes)\n",
        "\n",
        "# Create the model\n",
        "model = custom_resnet()\n",
        "\n",
        "# Summary of the model to check the total number of parameters\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ESZYQFp8XYP",
        "outputId": "c52a9f2a-3dba-4f21-85af-1624a77288bc"
      },
      "id": "1ESZYQFp8XYP",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-8           [-1, 64, 32, 32]             128\n",
            "              ReLU-9           [-1, 64, 32, 32]               0\n",
            "       BasicBlock-10           [-1, 64, 32, 32]               0\n",
            "           Conv2d-11           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-12           [-1, 64, 32, 32]             128\n",
            "             ReLU-13           [-1, 64, 32, 32]               0\n",
            "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
            "             ReLU-16           [-1, 64, 32, 32]               0\n",
            "       BasicBlock-17           [-1, 64, 32, 32]               0\n",
            "           Conv2d-18           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-19           [-1, 64, 32, 32]             128\n",
            "             ReLU-20           [-1, 64, 32, 32]               0\n",
            "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
            "             ReLU-23           [-1, 64, 32, 32]               0\n",
            "       BasicBlock-24           [-1, 64, 32, 32]               0\n",
            "           Conv2d-25          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-26          [-1, 128, 16, 16]             256\n",
            "             ReLU-27          [-1, 128, 16, 16]               0\n",
            "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
            "           Conv2d-30          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-31          [-1, 128, 16, 16]             256\n",
            "             ReLU-32          [-1, 128, 16, 16]               0\n",
            "       BasicBlock-33          [-1, 128, 16, 16]               0\n",
            "           Conv2d-34          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-35          [-1, 128, 16, 16]             256\n",
            "             ReLU-36          [-1, 128, 16, 16]               0\n",
            "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
            "             ReLU-39          [-1, 128, 16, 16]               0\n",
            "       BasicBlock-40          [-1, 128, 16, 16]               0\n",
            "           Conv2d-41          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-42          [-1, 128, 16, 16]             256\n",
            "             ReLU-43          [-1, 128, 16, 16]               0\n",
            "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
            "             ReLU-46          [-1, 128, 16, 16]               0\n",
            "       BasicBlock-47          [-1, 128, 16, 16]               0\n",
            "           Conv2d-48            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-49            [-1, 256, 8, 8]             512\n",
            "             ReLU-50            [-1, 256, 8, 8]               0\n",
            "           Conv2d-51            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-52            [-1, 256, 8, 8]             512\n",
            "           Conv2d-53            [-1, 256, 8, 8]          32,768\n",
            "      BatchNorm2d-54            [-1, 256, 8, 8]             512\n",
            "             ReLU-55            [-1, 256, 8, 8]               0\n",
            "       BasicBlock-56            [-1, 256, 8, 8]               0\n",
            "           Conv2d-57            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-58            [-1, 256, 8, 8]             512\n",
            "             ReLU-59            [-1, 256, 8, 8]               0\n",
            "           Conv2d-60            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-61            [-1, 256, 8, 8]             512\n",
            "             ReLU-62            [-1, 256, 8, 8]               0\n",
            "       BasicBlock-63            [-1, 256, 8, 8]               0\n",
            "           Conv2d-64            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-65            [-1, 256, 8, 8]             512\n",
            "             ReLU-66            [-1, 256, 8, 8]               0\n",
            "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
            "             ReLU-69            [-1, 256, 8, 8]               0\n",
            "       BasicBlock-70            [-1, 256, 8, 8]               0\n",
            "AdaptiveAvgPool2d-71            [-1, 256, 1, 1]               0\n",
            "           Linear-72                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 4,327,754\n",
            "Trainable params: 4,327,754\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 20.63\n",
            "Params size (MB): 16.51\n",
            "Estimated Total Size (MB): 37.15\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "images = torch.tensor(images)\n",
        "images = transform(images)\n",
        "\n",
        "# Since we don't have labels, using dummy labels for demonstration\n",
        "labels = torch.zeros(images.size(0), dtype=torch.long)\n",
        "\n",
        "# Create the DataLoader\n",
        "dataset = TensorDataset(images, labels)\n",
        "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Model, Loss, and Optimizer\n",
        "model = resnet18_modified().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, _) in enumerate(loader):\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.to(device))  # Using dummy labels here\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "zPbKju-xK5TC"
      },
      "id": "zPbKju-xK5TC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}